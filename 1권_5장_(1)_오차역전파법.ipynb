{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1권 5장 (1) 오차역전파법.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyObUHKYsEGxDI+6m+pnn31X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tirals88/Deep-Learning-from-Scratch/blob/main/1%EA%B6%8C_5%EC%9E%A5_(1)_%EC%98%A4%EC%B0%A8%EC%97%AD%EC%A0%84%ED%8C%8C%EB%B2%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#드라이브연동\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "WHXiB0epa7XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/My Drive/DLscratch/deep-learning-from-scratch/ch05'"
      ],
      "metadata": {
        "id": "i1AOQCYt29R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 오차역전파법\n",
        "\n",
        "앞 장에서는 신경망 학습에 대해서 설명을 하였다. 그 때는 가중치 매개변수의 손실함수의 기울기를 수치 미분을 사용하여 구하였다.\n",
        "수치미분은 구현이 쉽고 단순하지만 오래걸린다는 단점이 있었다. 이번에는 효율적으로 계산하는 '오차역전파법'을 배워볼 것이다.\n",
        "\n",
        "먼저 시작적 ( 계산 그래프를 통하여 )으로 이해를 해보자.\n",
        "\n",
        "오차역전파법을 계산 그래프로 설명한다는 생각은 **Andrej Karpathy**의 블로그, **Fei-Fei Li** 교수가 진행한 스탠퍼드 대학교의 딥러닝 수업 CS231n을 참고하였다고한다.\n",
        "물론 내가 정리한 모든 내용들 또한 **밑바닥부터 시작하는 딥러닝(Deep Learning from Scratch)**의 내용들이다."
      ],
      "metadata": {
        "id": "GcyC0jIxw5Yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 계산그래프\n",
        "\n",
        "그림을 통한 이해는 책을 통해 복습\n",
        "https://github.com/WegraLee/deep-learning-from-scratch/tree/master/ch05\n",
        "\n"
      ],
      "metadata": {
        "id": "rDdHMefly3FH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "계산 그래프의 특징은 '국소적 계산'을 전파함으로써 최종 결과를 얻는다는 점에 있다.\n",
        "\n",
        "국소적 계산은 결국 전체에서 어떤 일이 벌어지든 상관없이 독립적으로 자신과 관계된 정보만으로 결과를 출력할 수 있다는 것이다.\n",
        "\n",
        "👉 각 노드는 자신과 관련된 계산 외에는 아무것도 신경 쓸 게 없다.\n",
        "\n",
        "국소적 계산의 장점\n",
        "1. 전체가 아무리 복잡하더라도 각 노드에서는 단순한 계산에 집중하여 문제를 단순화 할 수 있다.\n",
        "2. 중간 계산 결과를 모두 보관할 수 있다.\n",
        "3. **역전파를 통해 '미분'을 효율적으로 계산할 수 있다.**\n",
        "\n"
      ],
      "metadata": {
        "id": "2JopivrK2vRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 연쇄법칙\n",
        "\n",
        "합성함수\n",
        "- 여러 함수로 굿어된 함수이다.\n",
        "$z = (x + y)^2$, 이 식은 아래의 두 개의 식으로 구성된 것이다.\n",
        "$$z = t^2$$\n",
        "$$t = x + y$$\n",
        "\n",
        "- 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타내진다.\n",
        "$$\\frac{\\partial z}{\\partial x} = \\frac{∂z}{∂t}\\frac{∂t}{∂x}$$\n",
        "\n",
        "위 식에서 $∂t$는 서로 지울 수 있다.\n",
        "\n",
        "$$\\frac{∂z}{∂t} = 2t$$\n",
        "\n",
        "$$\\frac{∂t}{∂x} = 1$$\n",
        "\n",
        "$$\\frac{\\partial z}{\\partial x} = \\frac{∂z}{∂t}\\frac{∂t}{∂x} = 2t ⋅1 = 2(x+y)$$\n",
        "\n",
        "- 국소적 미분의 전달\n",
        "\n",
        "$→ \\frac{\\partial z}{\\partial z}$ \n",
        "\n",
        "$→ \\frac{∂z}{∂z}\\frac{∂z}{∂t}$ \n",
        "\n",
        "$→ \\frac{\\partial z}{\\partial z}\\frac{∂z}{∂t}\\frac{∂t}{∂x}$\n",
        "\n",
        "이는 결국 $x$에 대한 $z$의 미분이 된다."
      ],
      "metadata": {
        "id": "IbniO36NCeYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 덧셈 노드의 역전파\n",
        "\n",
        "$z = x + y$에서 $\\frac{∂z}{∂x} = 1$, $\\frac{∂z}{∂y} = 1$이 된다.\n",
        "\n",
        "그러므로 덧셈 노드에서 역전파는 상류에서 전해진 미분 ( $\\frac{∂L}{∂z}$에 1을 곱하여 하류로 흘려보내는 것이다. 즉 1을 곱하기만 할 분이므로 입력된 값을 그대로 다음 노드로 흘려보내게 된다.\n",
        "\n",
        "덧셈 노드 역전파는 입력 신호를 다음 노드로 출력할 뿐이므로 그대로 다음 노드로 전달한다."
      ],
      "metadata": {
        "id": "eK4SC1RHETeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 곱셈 노드의 역전파\n",
        "\n",
        "$z = x ⋅ y$에서 $\\frac{∂z}{∂x} = y$, $\\frac{∂z}{∂y} = x$가 된다.\n",
        "\n",
        "곱셈 노드 역전파는 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 보낸다.\n",
        "\n",
        "**덧셈의 역전파는 상류의 값을 그대로 흘려보내기 대문에 순방향 입력 신호의 값은 필요하지 않았지만,**\n",
        "\n",
        "**곱셈의 역전파는 순방향 입력 신호의 값이 필요하다. 그래서 곱셈 노드를 구현할 때는 순전파의 입력 신호를 변수에 저장해둔다.**"
      ],
      "metadata": {
        "id": "bTz8KdCGH39u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 단순 계층 구현\n",
        "\n",
        "- 곱셈 노드 : MulLayer\n",
        "\n",
        "- 덧셈 노드 : AddLayer\n",
        "\n",
        "- 계층 : 신경망의 기능 단위. / 예를 들어 시그모이드 함수를 위한 Sigmoid, 행렬 곱을 위한 Affine 등이 해당 됨"
      ],
      "metadata": {
        "id": "y711AgHa5t3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 곱셈 계층\n",
        "\n",
        "모든 계층은 forward() / backward() 공통의 메서드(인터페이스)를 갖도록 구현할 것이다."
      ],
      "metadata": {
        "id": "z7v1qh5Q6H4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 곱셈 계층 \n",
        "\n",
        "class MulLayer():\n",
        "  def __init__(self):\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    out = x * y\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * self.y # x 와 y 를 바꿔 곱한다\n",
        "    dy = dout * self.x\n",
        "\n",
        "    return dx, dy"
      ],
      "metadata": {
        "id": "K910qxDv2NrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "# 계층들\n",
        "\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# 순전파\n",
        "\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "print(price) # 220"
      ],
      "metadata": {
        "id": "9-mDKDPn66LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 역전파\n",
        "\n",
        "dprice = 1\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(dapple, dapple_num, dtax)"
      ],
      "metadata": {
        "id": "tOLxMtwK914i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 덧셈 계층\n",
        "\n",
        "곱셈과 같이 forward() / backward() 구분"
      ],
      "metadata": {
        "id": "wGDNQiOyBbqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddLayer():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def forward(self, x, y):\n",
        "    out = x + y\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * 1\n",
        "    dy = dout * 1\n",
        "    return dx, dy"
      ],
      "metadata": {
        "id": "Bt902Ht4-48B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 활성화 함수 계층 구현\n",
        "\n",
        "- Relu\n",
        "\n",
        "- Sigmoid"
      ],
      "metadata": {
        "id": "lto-T7eCEM6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relu 계층\n",
        "\n",
        "활성화 함수로 사용되는 ReLU의 수식은 다음과 같다.\n",
        "\n",
        "$$y = \\begin{cases}x  (x > 0)\\\\0  (x\\leq0)\\end{cases}$$\n",
        "\n",
        "\n",
        "위 식에서 $x$ 에 대한 $y$ 의 미분은 아래와 같다.\n",
        "\n",
        "$$ \\frac{\\partial y}{\\partial x} = \\begin{cases}1 (x > 0)\\\\0 (x\\leq0)\\end{cases}$$\n",
        "\n",
        "위와 같이 순전파 때의 입력인 x가 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘린다.\n",
        "\n",
        "반면, 순전파 때 $x$가 0 이하면 역전파 때는 하류로 신호를 보내지 않는다. ( 0을 보낸다 )"
      ],
      "metadata": {
        "id": "KLfvJMT2EXHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ReLU 계층 구현\n",
        "\n",
        "class ReLU:\n",
        "  def __init__(self):\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.mask = ( x <= 0 ) # 0 보다 작은 원소들의 인수들을 True\n",
        "    out = x.copy()\n",
        "    out[self.mask] = 0  # 0 보다 작은 원소들 0으로 치환\n",
        "\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dout[self.mask] = 0\n",
        "    dx = dout\n",
        "\n",
        "    return dx"
      ],
      "metadata": {
        "id": "ZWio8mSCEWS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid 계층\n",
        "\n",
        "시그모이드 함수는 다음과 같다.\n",
        "$$y = \\frac{1}{1+\\exp(-x)}$$\n",
        "\n",
        "시그모이드 계층의 계산 그래프는 다음과 같다.\n",
        "1. $x$ / $-1$ 의 곱셈 계층 : $-x$\n",
        "2. $-x$ 의 exp 계층 : exp($-x$)\n",
        "3. exp($-x$)와 1의 더하기 계층 : 1 + exp($-x$)\n",
        "4. 1 + exp($-x$)의 나누기 계층 : $\\frac{1}{1+\\exp(-x)}$"
      ],
      "metadata": {
        "id": "BTK1AQV1T_Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1단계 '/' 노드, $y = \\frac{1}{x}$ 을 미분하면 다음 식이 된다.\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x} = -\\frac{1}{x^2} = -y^2$$\n",
        "\n",
        "위 식에 따라 역전파 때는 상류에서 흘러온 값에 $-y^2$ (순전파의 출력을 제곱한 후 마이너스를 붙인 값)을 곱해서 하류로 전달한다.\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial y} → -\\frac{\\partial L}{\\partial y}y^2$$\n",
        "\n",
        "- 2단계 '+' 노드, 상류의 값을 그대로 하류로 흘려보낸다. \n",
        "\n",
        "- 3단계 '$\\exp$'노드, $y = \\exp(x)\\ 연산을 수행하며, 그 미분은 다음과 같다.\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x} = \\exp(x)$$\n",
        "\n",
        "위 식에 따라 역전파 때는 상류에서 흘러온 값에 순전파의 출력 ($\\exp(-x)$)을 곱하여 하류로 흘려보낸다.\n",
        "\n",
        "$$-\\frac{\\partial L}{\\partial y}y^2 →-\\frac{\\partial L}{\\partial y}y^2\\exp(-x) $$\n",
        "\n",
        "- 4단계 'x' 노드, 순전파 때의 값을 '서로 바꿔' 곱하여 내보낸다.\n",
        "\n",
        "이 예에서는 '-1'을 곱한다.\n",
        "\n",
        "결국 최종 출력 값으로 $\\frac{\\partial L}{\\partial y}y^2\\exp(-x)$이 된다."
      ],
      "metadata": {
        "id": "RBR7PZFKO3sI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sigmoid 계층의 계산 그래프 ( 간소화 버전 )\n",
        "\n",
        "<img src = \"https://github.com/tirals88/Deep-Learning-from-Scratch/blob/main/deep_learning_images/fig%205-21.png?raw=true\">\n",
        "\n",
        "<출처 : \" https://github.com/WegraLee/deep-learning-from-scratch.git \">\n",
        "\n",
        "위와 같이 간소화 버전은 중간 계산들을 생략할 수 있어서 더 효율적인 계산이라 할 수 있다.\n",
        "\n",
        "또한 $\\frac{\\partial L}{\\partial y}y^2\\exp(-x)$는 다음처럼 정리해서 쓸 수도 있다.\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1CAxA5MMcP5ULT6nETykYv5fz9DSWNseZ\">\n",
        "\n",
        "위와 같이 Sigmoid 계층의 역전파는 순전파의 출력 ($y$)만으로 계산이 가능하다."
      ],
      "metadata": {
        "id": "CXuYKIwWXKMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid 계층 구현\n",
        "\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.out = None\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = 1 / (1 + np.exp(-x))\n",
        "    self.out = out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "    return dx\n",
        "  \n",
        "# 순전파의 출력을 인스턴스 변수 out에 보관했다가, 역전파 계산 때 그 값을 사용한다."
      ],
      "metadata": {
        "id": "w2FXe0QHY7Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Affine / Softmax 계층 구현\n",
        "\n",
        "## Affine 계층\n",
        "\n",
        "신경망의 순전파에서는 가중치 신호의 총합을 계산하기 때문에 행렬의 곱을 사용하였다.\n",
        "\n",
        "```\n",
        "X = np.random.rand(2)    # 입력\n",
        "W = np.random.rand(2, 3) # 가중치\n",
        "B = np.random.rand(3)    # 편향\n",
        "\n",
        "Y = np.dot(X, W) + B\n",
        "```\n",
        "신경망의 순전파 때 수행하는 행렬의 곱은 기하학에서는 **어파인 변환 affine transformation**이라고 한다.\n"
      ],
      "metadata": {
        "id": "dW1A2sGbeGf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전까지의 계산 그래프는 노드 사이에 '스칼라 값'이 흘렀는 데 반해, 이 예에서는 '행렬'이 흐르고 있다.\n",
        "\n",
        "행렬을 사용한 역전파도 행렬의 원소마다 전개해보면 스칼라값을 사용한 지금까지의 계산 그래프와 같은 순서로 생각할 수 있다.\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} ⋅ W^T$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial W} = X^T\\cdot\\frac{\\partial L}{\\partial Y}$$\n"
      ],
      "metadata": {
        "id": "3fytls5Th9T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YlhaWKNwbWiA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}