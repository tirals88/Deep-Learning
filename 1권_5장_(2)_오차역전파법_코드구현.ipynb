{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1권 5장 (2) 오차역전파법 코드구현.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMjbWr8a4bCA8GDij3Bj2mv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tirals88/Deep-Learning-from-Scratch/blob/main/1%EA%B6%8C_5%EC%9E%A5_(2)_%EC%98%A4%EC%B0%A8%EC%97%AD%EC%A0%84%ED%8C%8C%EB%B2%95_%EC%BD%94%EB%93%9C%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#드라이브연동\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "WHXiB0epa7XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/My Drive/DLscratch/deep-learning-from-scratch/ch05'"
      ],
      "metadata": {
        "id": "i1AOQCYt29R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8PfuZ2hXa4zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 코드의 전체 그림\n",
        "\n",
        "- 전체 : 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망은 아래 4단계로 수행이 된다.\n",
        "\n",
        "- 1단계 [ 미니배치 ] : 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라고 하며, 미니배치의 손실 함수를 줄이는 것이 목표이다.\n",
        "\n",
        "- 2단계 [ 기울기 산출 ] : 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
        "\n",
        "- 3단계 [ 매개변수 갱신 ] : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
        "\n",
        "- 4단계 : [ 반복 ] : 1~3단계를 반복한다.\n",
        "\n",
        "이 코드에서는 'numerical gradient'와 'backpropagation'모두를 구현하였다."
      ],
      "metadata": {
        "id": "FeJsFxDcfcPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "\n",
        "from common.layers import *   # 1권 5장 (1)에 코드 정리\n",
        "from common.gradient import numerical_gradient # 1권 4장 (2)에 코드 정리\n",
        "from collections import OrderedDict\n",
        "\n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size,\n",
        "               weight_init_std = 0.01):\n",
        "  \n",
        "    # 가중치 초기화\n",
        "\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * \\\n",
        "    np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * \\\n",
        "    np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    # 계층 생성\n",
        "\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers['Affine1'] = \\\n",
        "    Affine(self.params['W1'], self.params['b1'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Affine2'] = \\\n",
        "    Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "    self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  # x : input data / t : label data\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return self.lastLayer.forward(y, t)\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis = 1)\n",
        "    if t.ndimt != 1:\n",
        "      t = np.argmax(t, axis = 1)\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "  # x : input data / t : label data\n",
        "\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_w = lambda W : self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "    return grads\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    # 순전파\n",
        "    self.loss(x, t)\n",
        "\n",
        "    # 역전파\n",
        "    dout = 1\n",
        "    dout = self.lastLayer.backward(dout)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    #결과 저장\n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers['Affine1'].dW\n",
        "    grads['b1'] = self.layers['Affine1'].dW\n",
        "    grads['W2'] = self.layers['Affine2'].dW\n",
        "    grads['b2'] = self.layers['Affine2'].dW\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "Qi86ZIAqfXqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - 위와 같이 기울기를 구하는 방법은 수치 미분을 써서 구하는 방법과 해석적으로 수식을 풀어 구하는 오차역전파법이 있다.\n",
        "\n",
        " 1. 먼저 수치 미분의 이점은 구현이 쉬워 버그가 숨어있기 어렵다. 하지만 속도가 느리므로 효율이 떨어지게 된다.\n",
        "\n",
        " 2. 다음으로 오차역전파법은 속도가 수치 미분의 방식보다 빠른 것이 이점이다. 하지만 반대로 구현이 복잡하셔 실수가 종종 생기곤 한다.\n",
        "\n",
        " 3. 이를 위해 수치 미분은 학습에 사용이 되기 보다는 오차역전파법의 결과를 비교하여 제대로 구현하였는지 검증의 역할 **Gradient check**로 사용되고는 한다."
      ],
      "metadata": {
        "id": "1bbRXnEPGUZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "#데이터 읽기\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "load_mnist(normalize = True, one_hot_label = True)\n",
        "\n",
        "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
        "\n",
        "x_batch = x_train[:5]\n",
        "t_batch = t_train[:5]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "# 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n",
        "\n",
        "for key in grad_numerical.keys():\n",
        "  diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]))\n",
        "  print(key + \":\" + str(diff))"
      ],
      "metadata": {
        "id": "sGiROHIQkrp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위와 같은 결과를 보면 수치 미분과 오차역전파법으로 구한 기울기의 차이가 매우 작음을 알 수 있다.\n",
        "\n",
        "**결과 오차가 0이 되는 일은 드물다. 이는 컴퓨터의 부동소수점의 한계 때문에 0에 아주 가까운 값으로 나오기 때문이다.**"
      ],
      "metadata": {
        "id": "bK1doxUYJiFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 구현\n",
        "\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "load_mnist(normalize = True, one_hot_label = True)\n",
        "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  # 오차역전파법으로 기울기 계산\n",
        "  grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "  #갱신\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n",
        "  if i % (iter_per_epoch  / 4 ) == 0 :\n",
        "  #if i % 10 == 0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "\n",
        "    print(\"iters_num : \", i, \", train_acc : \", train_acc, \",  test_acc : \", test_acc)"
      ],
      "metadata": {
        "id": "nxQPheKgH_X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 100, iters_num = 10000\n",
        "axis_y2 = train_acc_list\n",
        "axis_y3 = test_acc_list\n",
        "\n",
        "axis_x = np.arange(len(train_acc_list)) * 150\n",
        "plt.plot(axis_x, axis_y2, label = 'train_acc')\n",
        "plt.plot(axis_x, axis_y3, label = 'test_acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y_z1zbJBUupn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 100, iters_num = 200\n",
        "axis_y2 = train_acc_list\n",
        "axis_y3 = test_acc_list\n",
        "\n",
        "axis_x = np.arange(len(train_acc_list)) * 5\n",
        "plt.plot(axis_x, axis_y2, label = 'train_acc')\n",
        "plt.plot(axis_x, axis_y3, label = 'test_acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7cxwqaqYXujc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}